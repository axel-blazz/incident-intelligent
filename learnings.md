# ðŸ“˜ Incident Intelligence Platform â€” Deep Learning Log

**Day 1 â†’ Day 8 (Grounded, Non-Hallucinated)**

This document records:

- The **exact questions/confusions** I had
- **Why I was confused**
- The **precise explanation that resolved it**
- The **mental model** I should remember

This is written for **revision**, not summary.

---

## ðŸŸ¦ Day 1 â€” PRD & Project Start

### Question I had

> â€œCan I just start building and refine later?â€

### Why I was confused

I was used to learning by coding first and fixing later.

### Explanation that helped

- PRD defines **what must exist**, **what must not exist**, and **what is intentionally postponed**
- Code written before understanding requirements usually needs rewriting

### Mental model

> PRD = boundary  
> Code = implementation inside that boundary

---

## ðŸŸ¦ Day 2 â€” Structure, Config & Logging

### Question I had

> â€œWhy do we need so many folders? Why not keep logic together?â€

### Why I was confused

Early projects work fine with everything mixed.

### Explanation that helped

Folders represent **change isolation**, not cleanliness.

- `schemas` â†’ API contract (changes with clients)
- `models` â†’ DB shape (changes with storage)
- `services` â†’ domain logic (changes with business)
- `routers` â†’ HTTP wiring (changes with API)

### Mental model

> Files are grouped by **reason to change**, not by type.

---

### Question I had

> â€œWhy did my Pydantic Settings class break FastAPI OpenAPI?â€

(Error: `info.title Input should be a valid string`)

### Why I was confused

I mixed `python-settings` and `pydantic.BaseSettings`.

### Explanation that helped

- FastAPI expects **plain strings** for OpenAPI metadata
- Passing `Field()` objects incorrectly leaks metadata instead of values
- Use **Pydantic Settings correctly**, donâ€™t mix libraries

### Mental model

> Config objects should expose **values**, not schema metadata.

---

## ðŸŸ¦ Day 3 â€” Schemas

### Question I had

> â€œWhy do we need UserIn, UserOut, UserPatch separately?â€

### Why I was confused

They all represent the same user.

### Explanation that helped

Each operation needs **different guarantees**:

- `UserIn` â†’ validation rules
- `UserOut` â†’ safe response (no password)
- `UserPatch` â†’ optional, partial updates

### Mental model

> Same entity, different **intent**, different **shape**

---

## ðŸŸ¦ Day 4 â€” Database & SQLAlchemy

### Question I had

> â€œI created a model â€” why is the table not created?â€

### Why I was confused

I assumed ORM models auto-create tables.

### Explanation that helped

- SQLAlchemy models **describe structure**
- Tables are created only when:
  - `Base.metadata.create_all()` runs, or
  - migrations are applied

### Mental model

> Model â‰  Table  
> ORM â‰  Database

---

### Question I had

> â€œWhy did `server_default=datetime.now()` break?â€

(Error: `ArgumentError: expected str or ClauseElement`)

### Explanation that helped

- `server_default` runs **in the database**
- Database cannot execute Python functions
- Must use `func.now()` or DB expressions

### Mental model

> `default=` â†’ Python  
> `server_default=` â†’ Database

---

### Question I had

> â€œWhy am I getting `Invalid isoformat string: 'now()'`?â€

### Explanation that helped

- SQLite doesnâ€™t understand Postgres-style `now()`
- SQLite stores timestamps as strings
- DB defaults must match DB dialect

### Mental model

> Defaults are **DB-specific**

---

## ðŸŸ¦ Day 5 â€” Authentication

### Question I had

> â€œWhy does Swagger OAuth UI ask for username/password when I use JWT?â€

### Why I was confused

Swagger UI â‰  actual auth flow.

### Explanation that helped

- Swagger OAuth UI is for OAuth2 Password Flow
- My system uses **JWT + HTTPBearer**
- Swagger UI is optional, not authoritative

### Mental model

> Swagger is a **testing tool**, not the auth system.

---

### Question I had

> â€œShould role be string if DB column is Enum?â€

### Explanation that helped

- In Python, use `Enum`
- In DB, store **string value**
- Convert explicitly at boundaries

### Mental model

> Enum for logic  
> String for storage

---

## ðŸŸ¦ Day 6 â€” Incident Domain

### Question I had

> â€œWhat do you mean by status transitions?â€

### Why I was confused

I thought status is just a field to update.

### Explanation that helped

- Incident lifecycle is **state-based**
- Not all transitions are valid
- Example:
  - OPEN â†’ INVESTIGATING âœ…
  - OPEN â†’ RESOLVED âŒ

### Mental model

> Status = state machine, not CRUD

---

### Question I had

> â€œShould IncidentStatus.OPEN and IncidentStatus.OPEN.value be the same?â€

### Explanation that helped

- `.OPEN` â†’ enum object
- `.OPEN.value` â†’ string
- Same value, different type

### Mental model

> Use enum in logic  
> Use `.value` for DB

---

## ðŸŸ¦ Day 7 â€” Async, PATCH & CRUD

### Question I had

> â€œIf my route is async and DB is sync, is it blocking?â€

### Explanation that helped

- FastAPI runs sync code in **threadpool**
- Event loop is not blocked
- Sync code still blocks **its thread**

### Mental model

> Async route â‰  async code  
> Threadpool protects event loop

---

### Question I had

> â€œShould auth functions also be async?â€

### Explanation that helped

- Auth logic is CPU-bound, fast
- No I/O â†’ no benefit from async

### Mental model

> Async is for I/O, not for everything

---

### Question I had

> â€œWhy does empty PATCH return 200?â€

### Explanation that helped

- `{}` becomes `IncidentPatch(status=None)`
- `payload is None` never triggers
- Must check **fields**, not object

### Mental model

> Empty PATCH = all fields None

---

### Question I had

> `if not var` vs `if var is None`?

### Explanation that helped

- `not var` checks **truthiness**
- `is None` checks **absence**
- PATCH requires absence detection

### Mental model

> PATCH cares about **provided vs not provided**

---

## ðŸŸ¦ Day 8 â€” Incident Logs & Relationships

### Question I had

> â€œWhere should relationship() be defined?â€

### Explanation that helped

- ForeignKey goes on **many side**
- relationship() goes where navigation is needed
- Relationship is **ORM-level**, not DB-level

### Mental model

> DB stores relation  
> ORM expresses navigation

---

### Question I had

> â€œWhy did relationship import fail?â€

### Explanation that helped

- `relationship` is in `sqlalchemy.orm`
- Not in `sqlalchemy`

### Mental model

> ORM tools live in `sqlalchemy.orm`

---

### Question I had

> â€œDoes ondelete='CASCADE' delete from parent or child?â€

### Explanation that helped

- Defined on **child**
- Triggered by **parent delete**
- Parent delete â†’ child rows auto-deleted

### Mental model

> Parent dies â†’ children cleaned

---

### Question I had

> â€œWhat is lazy vs eager loading?â€

### Explanation that helped

- Lazy â†’ load when accessed
- Eager â†’ load upfront
- N+1 problem happens with lazy loading in loops

### Mental model

> Lazy = on demand  
> Eager = in bulk

---

## ðŸ”‘ Core Mental Models I Must Retain

- Async protects event loop, not logic
- PATCH must reject empty intent
- Enum = domain truth, string = persistence
- ORM models donâ€™t create tables
- Relationships are navigation, not storage
- Cascade protects integrity
- Lazy loading can silently kill performance

---

## ðŸ§  Day 9 â€” Refactor, Relationships & Responsibility

> **Theme:** Make the system boring, predictable, and impossible to misuse.

Day 9 was focused entirely on **correctness and architectural discipline**, not on adding features.  
The goal was to refactor the system so future work (Kafka, workers, AI) becomes safe and easy.

---

### 1. Router â†’ Service â†’ Repository Separation

### Initial Confusion

- Route logic felt â€œtoo abstractedâ€
- Business logic was hard to locate
- Most logic looked like DB interaction
- Unsure why logic wasnâ€™t directly in routes

### What Was Wrong

- Routers were handling DB queries and commits
- Logic was spread across layers
- Hard to reuse code outside FastAPI
- Risky to extend for workers / Kafka

### Final Understanding

Each layer has a single responsibility:

#### Router

- HTTP handling only
- Authentication & role checks
- Calls service functions
- Translates domain errors to HTTP responses

#### Service

- Business rules & validations
- Orchestration logic
- Raises `ValueError` for domain failures
- No FastAPI imports

#### Repository

- SQLAlchemy queries
- Loading strategy (`selectinload`)
- Commits and refresh
- Returns objects or `None`
- No business or HTTP logic

**Key takeaway:**  
If logic should work outside HTTP, it does not belong in routers.

---

### 2. ORM Relationships â€” Core Concepts

### Questions Faced

- On which table should relationships be defined?
- Does `relationship()` create DB columns?
- What do `backref` and `back_populates` do?
- How do lazy and eager loading differ?

### Final Understanding

#### Database Reality

- Only `ForeignKey` creates DB relationships
- `relationship()` is ORM navigation only

ORM Navigation

```python
incident_id = Column(ForeignKey("incidents.id"))
logs = relationship(
    "IncidentLogDB",
    back_populates="incident"
)
```

Enables incident.logs and log.incident

Does not enforce DB constraints

---

### 3. backref vs back_populates

- backref creates implicit reverse links

- Harder to reason about

- Risky in large codebases

- back_populates is explicit

- Clear ownership on both sides

- Safer and preferred in production

**Final rule**: Always prefer back_populates.

---

### 4. Cascade Delete â€” ORM vs Database

Confusion

- â€œWhere do I set cascade=true for DB?â€

- Already used cascade="all" in relationship

Final Understanding

There are two different cascades:

**ORM-level cascade (Python safety)**

```python
relationship(
    cascade="all, delete-orphan",
    passive_deletes=True
)
```

- Prevents orphan objects

- Deletes logs when parent is deleted via ORM

**DB-level cascade (Database safety)**

```python
ForeignKey("incidents.id", ondelete="CASCADE")
```

- Enforced by the database

- Works even if ORM is bypassed

- Defined only on the child table

**Key takeaway:**
ORM cascade and DB cascade are different and both are required.

---

### 5. Lazy vs Eager Loading (N+1 Problem)

**The Problem (N+1)**

```python
incidents = db.query(IncidentDB).all()
for inc in incidents:
    inc.logs
```

- 1 query for incidents

- 1 additional query per incident

- Performance degrades rapidly

**The Solution (Eager Loading)**

```python
db.query(IncidentDB)
  .options(selectinload(IncidentDB.logs))
```

- Always 2 queries

- Predictable performance

- Loading strategy centralized in repositories

**Key takeaway:**
Loading strategy belongs in repositories, not services or routers.

---

### 6. Error Handling Normalization

### Initial Issues

- Services raised HTTPException
- Routers returned inconsistent status codes
- All exceptions mapped to 400
- Internal errors leaked to clients

### Final Understanding

**Services**

- Raise `ValueError` for domain errors
- Never import FastAPI
- Never return `None` on failure

**Routers**

- Translate errors to HTTP responses
- `ValueError("not found")` â†’ 404
- Other `ValueError` â†’ 400
- Uncaught `Exception` â†’ 500
- Rollback DB session on write failures

```python
except ValueError as ve:
  status = 404 if "not found" in str(ve).lower() else 400
  raise HTTPException(status_code=status, detail=str(ve))
except Exception:
  db.rollback()
  raise HTTPException(status_code=500, detail="Internal server error")
```

**Key takeaway:**  
Errors move up, responsibilities move down.

---

## ðŸŸ¦ Day 8 (continued) â€” PATCH Semantics

### Question I had

> "Why does empty PATCH payload return 200?"

### Why I was confused

I thought `{}` was a valid no-op request.

### Explanation that helped

- Empty PATCH signals client confusion or bug
- Must reject intent-less requests explicitly
- Enforce contract: **update intent required**

```python
if not any([patch.status, patch.priority, patch.assigned_to]):
  raise ValueError("No fields provided for update")
```

### Mental model

> Empty PATCH = 400  
> Explicit contract prevents silent bugs

---

### Question I had

> "Are sync DB calls blocking in async routes?"

### Explanation that helped

- FastAPI runs sync code in threadpool
- Event loop is not blocked
- Async DB is optional for this project

### Mental model

> Async route â‰  must use async DB

---

## ðŸŽ¯ What Day 9 Achieved

**Technical**

- Correct ORM relationships with explicit `back_populates`
- Safe cascade deletes at both ORM and DB levels
- Predictable query behavior via `selectinload` in repositories
- Clean error flow from domain to HTTP

**Architectural**

- Clear layer boundaries (Router â†’ Service â†’ Repository)
- Services reusable outside HTTP context
- System ready for Kafka, workers, and AI
- Reduced accidental complexity

## ðŸŸ¦ Day 10 â€” Event-Driven Architecture Fundamentals

**Theme:** Event-driven architecture fundamentals (without Kafka)  
Today was about designing contracts and boundaries, not infrastructure.

---

### Question I had

> "Isn't an event just another API payload?"

### Why I was confused

I thought events were similar to request/response schemas.

### Explanation that helped

An event is a **fact about the past**, not a command.

- âŒ `CreateIncident` (command)
- âœ… `IncidentCreated` (event)

An event:

- Is **immutable**
- Describes something that **already happened**
- Can be **safely replayed**
- Must make sense **on its own** (without DB access)

### Mental model

> Events are immutable facts, not mutable commands.

---

### Question I had

> "Why not just use Kafka directly?"

### Why I was confused

I wanted to skip the in-memory phase and go straight to infrastructure.

### Explanation that helped

Kafka is **transport**, not design.

Starting in-memory helped me:

- Freeze event schemas early
- Understand when events should be emitted
- Avoid coupling business logic to Kafka APIs
- Debug events easily via logs

Kafka later becomes a plug-in, not a rewrite.

### Mental model

> Design events first, transport second.

---

### Question I had

> "Why not put events inside `schemas/`?"

### Why I was confused

I thought all schemas belonged in one place.

### Explanation that helped

| Aspect    | API Schemas       | Event Schemas      |
| --------- | ----------------- | ------------------ |
| Scope     | External contract | Internal contract  |
| Transport | Tied to HTTP      | Transport-agnostic |
| Stability | Changes often     | Must be stable     |
| Audience  | Client-facing     | System-facing      |

Created `app/events/` for this separation to prevent future chaos.

### Mental model

> API contracts â‰  Event contracts

---

### Question I had

> "Why does every event need the same metadata?"

### Why I was confused

Metadata felt like overhead.

### Explanation that helped

Shared metadata enables:

- **Idempotency** (duplicate detection)
- **Debugging** (trace events)
- **Ordering** (causality)
- **Kafka routing** (partition keys)

Key fields that must always exist:

- `event_id`
- `event_type`
- `occurred_at`
- `source`

Also learned why:

```python
class Config:
  frozen = True
```

Events must never be mutated.

### Mental model

> Events = immutable facts with traceable metadata

---

### Question I had

> "Why not emit events from routers?"

### Why I was confused

I thought events could be emitted anywhere in the stack.

### Explanation that helped

| Layer       | Emit events? | Why                       |
| ----------- | :----------: | ------------------------- |
| Router      |      âŒ      | Knows HTTP, not business  |
| Repository  |      âŒ      | Knows DB, not intent      |
| **Service** |      âœ…      | Knows business milestones |
| Mapper      |      âŒ      | Pure translation          |
| Dispatcher  |      âŒ      | Transport only            |

Services are the only correct place.

### Mental model

> Only services understand business intent.

---

### Question I had

> "Why not just log the event directly?"

### Why I was confused

I saw the event dispatcher as unnecessary abstraction.

### Explanation that helped

The dispatcher:

- Hides **how** events are transported
- Allows **swapping** logging â†’ Kafka later
- Keeps services **clean and future-proof**

This line became the only thing services care about:

```python
event_dispatcher.emit(event)
```

### Mental model

> Abstraction hides transport, not intent.

---

### Question I had

> "Isn't this extra indirection?"

(referring to mapper functions)

### Explanation that helped

Mapper functions:

- Prevent duplication
- Centralize event creation logic
- Make services simpler
- Make Kafka migration painless
- Are easy to unit test

Key rule I learned:

> Mappers are **pure functions**. No DB, no logging, no dispatching.

### Mental model

> Mappers translate, never mutate or emit.

---

### Question I had

> "When exactly should an event be emitted?"

### Why I was confused

I wasn't sure if timing mattered.

### Explanation that helped

Always **after DB commit**. Never before.

Correct order:

```
DB commit â†’ create event â†’ emit event
```

Why this matters:

- Prevents **phantom events** (not persisted)
- Enables **retries** later
- Makes **idempotency** possible

### Mental model

> Commit first, emit second.

---

### Question I had

> "Are all events equal?"

### Why I was confused

I thought events were monolithic.

### Explanation that helped

| Event Type               | Visibility | Frozen? |
| ------------------------ | ---------- | ------- |
| `IncidentCreatedEvent`   | Public     | âœ…      |
| `LogAttachedEvent`       | Public     | âœ…      |
| `AnalysisRequestedEvent` | Private    | â“      |

Only freeze events once you are sure:

- Downstream systems will depend on them
- You won't regret the contract

### Mental model

> Freeze contracts only when stable.

---

## ðŸ”‘ Core Insights from Day 10

- Events are immutable facts about the past, not commands
- Event schemas â‰  API schemas (different stability needs)
- Services emit events, never routers or repositories
- Event dispatcher abstracts transport (Kafka becomes plug-in)
- Emission timing: always after DB commit
- Freeze events only when downstream systems depend on them

---

## ðŸŸ¦ Day 11 â€” Kafka Integration (Producer + Mental Model)

### What I built

- Integrated Kafka as an event backbone
- Replaced in-memory event dispatch with Kafka producer
- Verified events using Kafka CLI consumer
- Designed consumer behavior before writing worker code

---

### Key concepts I learned

#### 1. Kafka is NOT a queue â€” it is a log

- Events are **appended** to a topic
- Kafka does not â€œdeleteâ€ messages after consumption
- Consumers track their own progress via **offsets**

This clarified why:

- Multiple consumers can read the same event
- Replaying events is possible
- Kafka suits event-driven systems, not task queues

---

#### 2. Broker, Topic, Event (clear separation)

- **Broker** â†’ Kafka server that stores data
- **Topic** â†’ named append-only log (e.g. `incident.events`)
- **Event** â†’ immutable record (JSON payload)

Kafka itself does not understand my schema â€” it only stores bytes.

---

#### 3. Why producers are simple (and consumers are hard)

- Producer:
  - Serialize
  - Send
  - Done
- Consumer:
  - Poll
  - Deserialize
  - Validate
  - Process
  - Commit offset
  - Handle retries and failures

Most complexity lives on the **consumer side**, not producer.

---

#### 4. Why we designed consumers BEFORE writing code

I learned that jumping straight to writing a consumer leads to:

- Duplicate processing
- Lost events
- Infinite retry loops

Designing first helped clarify:

- When to commit offsets
- Which failures are retryable
- How unknown events should be handled

---

#### 5. Kafka CLI consumer is essential

Using `kafka-console-consumer` taught me:

- Kafka really stores events independently of my app
- Events persist even if the API is stopped
- Debugging Kafka is much easier via CLI than code

---

### Things I got stuck on (and resolved)

#### âŒ Kafka/Zookeeper docker issues

- Learned that Docker Desktop engine issues can prevent Kafka startup
- Understood that Kafka infra must be stable before app debugging

---

### What I now understand clearly

- Kafka = durable event log
- Producers donâ€™t guarantee processing â€” consumers do
- Offsets are Kafkaâ€™s memory, not mine

---

## ðŸŸ¦ Day 12 â€” Kafka Consumer & Worker Service

### What I built

- A **separate worker process**
- Kafka consumer with manual offset control
- Event deserialization & validation
- Event routing by `event_type`
- Correct offset commit discipline

---

### Key concepts I learned

#### 1. Worker â‰  FastAPI â‰  background task

The worker is:

- A long-running process
- Started independently (`python consumer.py`)
- Completely decoupled from the API

This clarified real async system architecture.

---

#### 2. Offset = â€œHow far the consumer group has safely progressedâ€

Offsets are:

- Numbers in Kafka partitions
- Stored in Kafkaâ€™s internal `__consumer_offsets` topic
- Tracked per **consumer group**

Calling `consumer.commit()` tells Kafka:

> â€œEverything up to this message is safely processed.â€

---

#### 3. Why auto-commit is dangerous

With auto-commit:

- Kafka may mark events as processed **before my code runs**
- Crashes cause silent data loss

Manual commit ensures:

- At-least-once delivery
- Crash safety
- Predictable retries

---

#### 4. Correct offset commit strategy

I learned the correct pattern:

- Commit only after success
- No commit on handler failure
- Commit invalid or unknown events to avoid infinite loops

---

#### 5. Deserialization belongs in the consumer

Kafka delivers **bytes**, not valid objects.

I learned to:

- Safely decode JSON
- Reject malformed payloads
- Validate minimum required fields
- Prevent worker crashes due to bad data

---

#### 6. Event routing is my responsibility

Kafka does not route events.

Routing logic:

- Based on `event_type`
- Implemented via a handler map
- Keeps consumer loop clean and readable

This avoids `if/else` chaos and makes the system extensible.

---

#### 7. Retry behavior is controlled by commits

I finally understood:

- âŒ Crash before commit â†’ Kafka retries
- âœ… Commit after success â†’ Kafka moves forward
- âŒ Commit too early â†’ data loss

Retries are a **feature**, not a bug.

---

### Things I got stuck on (and resolved)

#### âŒ â€œDoes committing make sync code async?â€

Learned that:

- Commit does NOT affect async behavior
- It only affects Kafkaâ€™s replay logic
- Sync handlers still block, but Kafka safety remains intact

---

#### âŒ â€œShould everything be async?â€

Learned:

- Kafka consumer loop can remain sync
- Async DB/AI decisions are independent
- Correctness > async hype

---

#### âŒ â€œWhy commit unknown events?â€

Learned:

- Unknown events are non-retryable
- Not committing causes infinite reprocessing
- Logging + commit is the correct behavior

---

### What I now understand clearly

- Kafka guarantees **delivery**, not processing
- Offset commits define correctness
- Worker stability matters more than speed
- Event-driven systems demand discipline

---

## ðŸ Summary (Day 11â€“12)

By the end of Day 12, I moved from:

> â€œI can send eventsâ€

to:

> â€œI can safely process events in a distributed system.â€

I now understand:

- Kafka internals at a practical level
- Consumer groups and offsets
- Failure handling patterns
- Why event-driven systems are hard but powerful

# ðŸŸ¦ Day 13 â€” Redis Caching (Design, Providers & Invalidation)

### Theme

Introduce Redis as a **performance optimization layer** while maintaining DB as source of truth and clear separation of concerns.

---

### Question I had

> "Can I just inject Redis like a DB session using `Depends()`?"

### Why I was confused

I thought all dependencies followed the same FastAPI pattern.

### Explanation that helped

DB sessions and Redis clients have different lifecycles:

| Dependency     | Lifecycle   | Reason                            |
| -------------- | ----------- | --------------------------------- |
| DB Session     | Per request | Isolation, transactions, rollback |
| Redis Client   | Per process | Stateless, long-lived connection  |
| Kafka Producer | Per process | Infrastructure-level dependency   |

Request-scoped dependencies use `yield` + `Depends()`.
Process-scoped dependencies use **providers**.

### Mental model

> Different lifetimes require different patterns.

---

### Question I had

> "Should I instantiate Redis in `__init__.py`?"

### Why I was confused

I wanted a central place to create all infrastructure.

### Explanation that helped

Instantiating at import time causes:

- Side effects before app is ready
- Test failures (Redis not available)
- CLI scripts breaking unexpectedly

Use **lazy providers** instead:

```python
# cache/provider.py
_backend = None

def get_cache_backend():
  global _backend
  if _backend is None:
    _backend = RedisCacheBackend()
  return _backend
```

This pattern works for:

- Redis cache
- Event dispatcher
- Kafka producer

### Mental model

> Instantiate at use time, not import time.

---

### Question I had

> "Why abstract the cache backend?"

### Why I was confused

Redis seemed simple enough to use directly in services.

### Explanation that helped

Abstraction enables:

- Swap Redis â†’ in-memory â†’ mock for testing
- No Redis imports in business logic
- Graceful degradation in failures

Services only know:

```python
cache.get(key)
cache.set(key, value, ttl)
cache.delete(key)
```

### Mental model

> Abstract infrastructure, hide implementation.

---

### Question I had

> "How do I handle UUIDs as cache keys?"

### Why I was confused

Redis keys are strings, but my IDs are UUID objects.

### Explanation that helped

Conversion happens at boundaries:

- Domain logic: work with UUIDs
- Cache layer: convert to strings
- Backend: store strings

```python
cache_key = f"incident:{str(incident_id)}"
```

Backend remains generic and unaware of domain types.

### Mental model

> Type conversion at layer boundaries.

---

### Question I had

> "When do we delete cached data?"

### Why I was confused

I thought TTL was enough.

### Explanation that helped

TTL is a **safety net**, not a strategy.

Invalidate explicitly on:

- Incident update
- Log attachment
- Incident delete

```python
save(db, incident)
delete_from_cache(incident_id)
```

### Mental model

> Writes invalidate. Reads populate.

---

### Question I had

> "What is the correct order: DB â†’ Cache â†’ Events?"

### Why I was confused

I wasn't sure if ordering mattered.

### Explanation that helped

Always follow this order:

1. Commit DB transaction
2. Invalidate cache
3. Emit event

Why:

- If DB fails â†’ cache untouched (no stale data)
- If cache fails â†’ events still valid
- Workers always see committed state

### Mental model

> Transactions first, side effects second.

---

### Question I had

> "Does `save()` modify the object in place or create a copy?"

### Why I was confused

I wasn't sure if I needed to reload objects from DB.

### Explanation that helped

SQLAlchemy tracks the same Python object:

- Modify: `incident_db.status = "CLOSED"`
- Flush: `db.flush()` (validates)
- Commit: `db.commit()` (persists)
- Refresh: `db.refresh(incident_db)` (reload if needed)

The same object is updated through the lifecycle. No copies unless explicitly created.

### Mental model

> SQLAlchemy tracks objects, not rows.

---

### Question I had

> "Why can't I use read-through cache everywhere?"

### Why I was confused

Read-through cache seemed like a universal optimization.

### Explanation that helped

Read-through cache pattern:

1. Try cache
2. If miss â†’ query DB
3. Populate cache
4. Return

```python
cached = get_from_cache(id)
if cached:
  return cached

incident = get_from_db(id)
set_in_cache(id, incident)
return incident
```

Limits:

- Only for reads that repeat
- Not for frequently changing data
- Cache never becomes source of truth

### Mental model

> Cache optimizes reads, DB ensures correctness.

---

## ðŸ”‘ Core Mental Models (Day 13)

- DB sessions are request-scoped; Redis is process-scoped
- Lazy providers prevent import-time side effects
- Abstract infrastructure to enable testing
- Invalidate explicitly when data changes
- Transaction order: DB â†’ Cache â†’ Events
- Same Python object tracks through SQLAlchemy lifecycle
- Cache is optimization, never source of truth
